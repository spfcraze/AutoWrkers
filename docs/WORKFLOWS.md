# Multi-LLM Workflow Pipeline

UltraClaude's workflow system lets you orchestrate multiple AI models in a collaborative pipeline for code analysis, implementation, and review.

## Table of Contents

- [Overview](#overview)
- [Concepts](#concepts)
- [Creating Workflows](#creating-workflows)
- [Running Workflows](#running-workflows)
- [Provider Configuration](#provider-configuration)
- [Templates](#templates)
- [API Reference](#api-reference)

---

## Overview

The Multi-LLM Workflow Pipeline enables you to:

- **Chain multiple AI models**: Use different models for different tasks (e.g., Gemini for analysis, Claude for implementation, GPT-4 for review)
- **Run phases in parallel**: Execute independent phases concurrently
- **Track costs**: Monitor token usage and costs across all providers
- **Iterate on failures**: Automatically retry failed phases with feedback
- **Manage artifacts**: Pass outputs between phases automatically

### Example Pipeline

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│     Gemini      │    │     Claude      │    │     GPT-4       │
│    (Analyze)    │───▶│  (Implement)    │───▶│    (Review)     │
│  Fast, cheap    │    │  High quality   │    │   Critical eye  │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                      │                      │
         ▼                      ▼                      ▼
   Analysis Report       Code Changes          Review Report
```

---

## Concepts

### Workflow Template

A reusable definition of a workflow, containing:
- **Phases**: Ordered list of steps to execute
- **Budget limit**: Maximum cost for the workflow
- **Iteration settings**: How to handle failures
- **Failure behavior**: What to do when a phase fails

### Workflow Execution

A running instance of a template with:
- **Status**: pending, running, paused, completed, failed, cancelled
- **Phase executions**: Results from each phase
- **Artifacts**: Outputs generated by phases
- **Cost tracking**: Tokens and costs accumulated

### Phase

A single step in a workflow with:
- **Provider config**: Which LLM to use and how
- **Role**: analyzer, implementer, reviewer, validator
- **Prompt template**: Instructions for the LLM
- **Dependencies**: Input artifacts from previous phases

### Artifact

An output from a phase:
- **Types**: analysis, code, review, plan, diff, test_results
- **Content**: The actual text/code output
- **Metadata**: Phase info, token counts, etc.

---

## Creating Workflows

### Via Web UI

1. Navigate to the **Workflows** page
2. Click **New Template**
3. Configure the template:
   - **Name**: Descriptive name for the template
   - **Description**: What this workflow does
   - **Budget Limit**: Maximum cost in USD (optional)

4. Add phases:
   - Click **Add Phase**
   - Select a **Provider** (Gemini, OpenAI, Claude, Ollama, LM Studio)
   - Select a **Model**
   - Choose a **Role** (analyzer, implementer, reviewer, validator)
   - Write the **Prompt Template**

5. Click **Save Template**

### Via API

```bash
curl -X POST http://localhost:8420/api/workflow/templates \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Code Review Pipeline",
    "description": "Three-model code review workflow",
    "phases": [
      {
        "name": "Analysis",
        "order": 1,
        "role": "analyzer",
        "provider_config": {
          "provider_type": "gemini",
          "model": "gemini-1.5-flash",
          "temperature": 0.3
        },
        "prompt_template": "Analyze the following code for potential issues:\n\n{task_description}"
      },
      {
        "name": "Implementation",
        "order": 2,
        "role": "implementer",
        "provider_config": {
          "provider_type": "openai",
          "model": "gpt-4-turbo-preview",
          "temperature": 0.2
        },
        "prompt_template": "Based on the analysis:\n{artifacts.Analysis}\n\nImplement the fixes."
      }
    ],
    "budget_limit": 1.00
  }'
```

---

## Running Workflows

### From Sessions Page

1. Select an active session
2. Click the **Review** button
3. Choose a workflow template
4. Optionally specify a focus area
5. Click **Start Review**

### From Workflows Page

1. Navigate to the **Workflows** page
2. Click **New Execution**
3. Select a template
4. Enter task description
5. Set project path
6. Click **Create & Run**

### Via API

```bash
# Create execution
curl -X POST http://localhost:8420/api/workflow/executions \
  -H "Content-Type: application/json" \
  -d '{
    "template_id": "template-123",
    "task_description": "Review the authentication module",
    "project_path": "/home/user/myproject"
  }'

# Run execution
curl -X POST http://localhost:8420/api/workflow/executions/{id}/run
```

### Monitoring Progress

The Workflows page shows:
- **Execution list**: All workflow runs with status
- **Pipeline view**: Visual representation of phases
- **Phase details**: Click a phase to see output and artifacts
- **Budget bar**: Current spend vs. limit

Real-time updates via WebSocket show phase progress as it happens.

---

## Provider Configuration

### Gemini (Google)

```json
{
  "provider_type": "gemini",
  "model": "gemini-1.5-flash",
  "api_key": "your-api-key",
  "temperature": 0.3,
  "max_tokens": 8192
}
```

**Models available:**
- `gemini-1.5-flash` - Fast and cheap
- `gemini-1.5-pro` - More capable
- `gemini-2.0-flash` - Latest model

**Setup:**
1. Get API key from [Google AI Studio](https://aistudio.google.com)
2. Add to workflow provider settings

### OpenAI (GPT-4)

```json
{
  "provider_type": "openai",
  "model": "gpt-4-turbo-preview",
  "api_key": "your-api-key",
  "temperature": 0.2,
  "max_tokens": 4096
}
```

**Models available:**
- `gpt-4-turbo-preview` - Latest GPT-4
- `gpt-4` - Standard GPT-4
- `gpt-3.5-turbo` - Faster, cheaper

**Setup:**
1. Get API key from [OpenAI Platform](https://platform.openai.com)
2. Add to workflow provider settings

### Ollama (Local)

```json
{
  "provider_type": "ollama",
  "model": "llama3.2:latest",
  "api_url": "http://localhost:11434",
  "temperature": 0.3
}
```

**Setup:**
```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull a model
ollama pull llama3.2:latest

# Ollama runs automatically after install
```

### LM Studio (Local)

```json
{
  "provider_type": "lm_studio",
  "model": "auto",
  "api_url": "http://localhost:1234/v1",
  "temperature": 0.3
}
```

**Setup:**
1. Download [LM Studio](https://lmstudio.ai)
2. Download a model from the Discover tab
3. Start the local server (Server tab)

---

## Templates

### Built-in Templates

UltraClaude includes pre-configured templates:

#### Code Review (3-Phase)

1. **Gemini Analysis**: Quick code analysis, identify issues
2. **Claude Implementation**: Fix identified issues
3. **GPT-4 Review**: Final review and approval

#### Security Audit

1. **GPT-4 Security Scan**: Identify vulnerabilities
2. **Claude Remediation**: Implement fixes
3. **Gemini Verification**: Verify fixes

#### Documentation Generator

1. **Gemini Outline**: Generate documentation structure
2. **Claude Write**: Write detailed documentation

### Creating Custom Templates

Best practices:

1. **Start simple**: Begin with 2-3 phases
2. **Use appropriate models**: 
   - Fast/cheap for initial analysis (Gemini Flash)
   - High-quality for implementation (Claude, GPT-4)
   - Critical for final review (GPT-4)
3. **Set reasonable budgets**: Track costs initially
4. **Use clear prompts**: Be specific about expected output
5. **Reference artifacts**: Use `{artifacts.PhaseName}` to pass outputs

### Prompt Template Variables

Available in prompt templates:

| Variable | Description |
|----------|-------------|
| `{task_description}` | The task/issue description |
| `{project_path}` | Path to the project |
| `{artifacts.PhaseName}` | Output from a previous phase |
| `{iteration}` | Current iteration number |

Example:
```
Review the following code analysis:

{artifacts.Analysis}

Based on this analysis, implement fixes for the identified issues.
Focus on: {task_description}

Project location: {project_path}
```

---

## API Reference

### Templates

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/workflow/templates` | List all templates |
| POST | `/api/workflow/templates` | Create template |
| GET | `/api/workflow/templates/{id}` | Get template |
| PUT | `/api/workflow/templates/{id}` | Update template |
| DELETE | `/api/workflow/templates/{id}` | Delete template |

### Executions

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/workflow/executions` | List executions |
| POST | `/api/workflow/executions` | Create execution |
| GET | `/api/workflow/executions/{id}` | Get execution |
| POST | `/api/workflow/executions/{id}/run` | Start execution |
| POST | `/api/workflow/executions/{id}/cancel` | Cancel execution |
| POST | `/api/workflow/executions/{id}/resume` | Resume paused execution |

### Artifacts

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/workflow/executions/{id}/artifacts` | Get execution artifacts |
| GET | `/api/workflow/artifacts/{id}` | Get specific artifact |

---

## Troubleshooting

### "Provider not available"

- Check API key is configured
- For local providers, ensure service is running:
  ```bash
  # Ollama
  curl http://localhost:11434/api/tags
  
  # LM Studio
  curl http://localhost:1234/v1/models
  ```

### "Budget exceeded"

- Increase budget limit in template
- Use cheaper models for initial phases
- Reduce max_tokens in provider config

### "Phase timeout"

- Increase timeout in provider config
- Use faster models (e.g., Gemini Flash)
- Reduce complexity of prompts

### "Artifact not found"

- Ensure previous phase completed successfully
- Check artifact name matches exactly (case-sensitive)
- Verify phase order is correct

---

## Best Practices

1. **Start with Gemini Flash** for initial analysis - fast and cheap
2. **Use Claude/GPT-4** for implementation - best code quality
3. **Set budget limits** to prevent runaway costs
4. **Monitor first few runs** to tune token limits
5. **Use parallel phases** for independent tasks
6. **Keep prompts focused** - one clear task per phase
7. **Test templates** on small tasks before production use
